{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c1df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6afe1",
   "metadata": {},
   "source": [
    "## Implementation of scaled dot product attention\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V $$\n",
    "With $Q$ the query, $K$ the key and $V$ the value as follows:\n",
    "\n",
    "$$ Q = W_q \\times X $$\n",
    "$$ K = W_k \\times X $$\n",
    "$$ V = W_v \\times X $$\n",
    "\n",
    "Suppose that $X \\in \\mathbb{R}^{N \\times d_x}$, $W_q \\in \\mathbb{R}^{d_q \\times d_x}$, $W_k \\in \\mathbb{R}^{d_k \\times d_x}$, $W_v \\in \\mathbb{R}^{d_v \\times d_x}$. Then the scaled dot product attention belong to $\\mathbb{R}^{N \\times d_v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5bfe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor]=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -math.inf)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(p_attn, v), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c4221",
   "metadata": {},
   "source": [
    "Example for three sequences and embeddings of size 3:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "    x_1 & x_2 & x_3 \\\\\n",
    "    x_4 & x_5 & x_6 \\\\\n",
    "    x_7 & x_8 & x_9 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We have the following matrices:\n",
    "\n",
    "$$ W_q = \\begin{bmatrix}\n",
    "    q_1 & q_2 & q_3 \\\\\n",
    "    q_4 & q_5 & q_6 \\\\\n",
    "    q_7 & q_8 & q_9 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ W_k = \\begin{bmatrix}\n",
    "    k_1 & k_2 & k_3 \\\\\n",
    "    k_4 & k_5 & k_6 \\\\\n",
    "    k_7 & k_8 & k_9 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ W_v = \\begin{bmatrix}\n",
    "    v_1 & v_2 & v_3 \\\\\n",
    "    v_4 & v_5 & v_6 \\\\\n",
    "    v_7 & v_8 & v_9 \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85dfc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "Q = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "K = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "V = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "v, attn = scaled_dot_product(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03be90fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.4047e-10, 3.0667e-05, 9.9997e-01],\n",
      "        [2.7127e-23, 5.2083e-12, 1.0000e+00],\n",
      "        [7.8241e-37, 8.8454e-19, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483200a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.2083e-12, 1.0000e+00, 0.0000e+00],\n",
      "        [7.8241e-37, 8.8454e-19, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "## With mask\n",
    "\n",
    "mask = torch.tensor([[1, 0, 0],\n",
    "                     [1, 1, 0],\n",
    "                     [1, 1, 1]],\n",
    "                     dtype=torch.float32\n",
    "                     )\n",
    "v, attn = scaled_dot_product(Q, K, V, mask)\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80573f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W_k = nn.Linear(d_in, d_in)\n",
    "        self.W_q = nn.Linear(d_in, d_in)\n",
    "        self.W_v = nn.Linear(d_in, d_in)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None):\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        return scaled_dot_product(Q, K, V, mask)\n",
    "\n",
    "\n",
    "class MultiHeadAttention1(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_in: int):\n",
    "        super(MultiHeadAttention1, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = nn.ModuleList([Attention(d_in) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(n_heads * d_in, d_in)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "\n",
    "        output = torch.cat([attn(query, key, value, mask)[0] for attn in self.attention], dim=-1)\n",
    "        output = self.linear(output)\n",
    "        return  output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "619f59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_model: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        B, T, D = query.size()\n",
    "\n",
    "        # Linear projections\n",
    "        print(f\"query shape: {query.shape} key shape: {key.shape} value shape: {value.shape}\")\n",
    "        Q = self.W_q(query).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # [B, H, T, d_head]\n",
    "        K = self.W_k(key).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(value).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)  # [B, H, T, d_head]\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, D)  # [B, T, D]\n",
    "        return self.out_proj(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b7b2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "attn = Attention(3)\n",
    "query = torch.randn(2, 5, 3)\n",
    "key = torch.randn(2, 5, 3)\n",
    "value = torch.randn(2, 5, 3)\n",
    "\n",
    "output = attn(query, key, value)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4169bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "n_heads = 4\n",
    "\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "mha = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
    "output = mha(query, key, value)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b81bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.pe[:, :x.size(1)]  # type: ignore\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f37e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Random input tensor [batch_size, seq_len, d_model]\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Instantiate and run PositionalEncoding\n",
    "pos_encoder = PositionalEncoding(d_model)\n",
    "x_encoded = pos_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dab03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ffn),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ffn, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "482b607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model)\n",
    "        self.ffn = FFN(d_model, d_ffn, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85d6fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "Input shape: torch.Size([2, 5, 16])\n",
      "Output shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "n_heads = 4\n",
    "d_ffn = 64\n",
    "encoder = EncoderLayer(d_model=d_model, n_heads=n_heads, d_ffn=d_ffn)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = encoder(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20a2fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ffn, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.mha2 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.ffn = FFN(d_model, d_ffn, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        _x = x\n",
    "        x = self.mha1(x, x, x, tgt_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        _x = x\n",
    "        x = self.mha2(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95520164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "Input shape: torch.Size([2, 5, 16])\n",
      "Output shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "Input shape: torch.Size([2, 5, 16])\n",
      "Output shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "n_heads = 4\n",
    "d_ffn = 64\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "encoder = EncoderLayer(d_model, n_heads, d_ffn)\n",
    "\n",
    "output = encoder(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "decoder = DecoderLayer(d_model, n_heads, d_ffn)\n",
    "\n",
    "output = decoder(x, x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b615e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
    "                 d_ffn: int, num_layers: int, dropout: float=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.token_embd = nn.Embedding(vocab_size, d_model)\n",
    "        self.embd = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ffn, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.token_embd(x)\n",
    "        x = self.embd(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "888cfaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
    "                 d_ffn: int, num_layers: int, dropout: float=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.token_embd = nn.Embedding(vocab_size, d_model)\n",
    "        self.embd = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ffn, dropout)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.token_embd(x)\n",
    "        x = self.embd(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28b47144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_vocab_size: int,dec_vocab_size: int, d_model: int,\n",
    "                 n_heads: int, d_ffn: int, num_layers: int, dropout: float=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size=enc_vocab_size, d_model=d_model, n_heads=n_heads,\n",
    "                               d_ffn=d_ffn, num_layers=num_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(vocab_size=dec_vocab_size, d_model=d_model, n_heads=n_heads,\n",
    "                               d_ffn=d_ffn, num_layers=num_layers, dropout=dropout)\n",
    "        self.out_proj = nn.Linear(d_model, dec_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        return self.out_proj(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88da7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "query shape: torch.Size([2, 5, 16]) key shape: torch.Size([2, 5, 16]) value shape: torch.Size([2, 5, 16])\n",
      "Input shape: torch.Size([2, 5])\n",
      "Output shape: torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "n_heads = 4\n",
    "d_ffn = 64\n",
    "num_layers = 2\n",
    "vocab_size = 100\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # token indices for encoder input\n",
    "y = torch.randint(0, vocab_size, (batch_size, seq_len))  # token indices for decoder input\n",
    "\n",
    "transformer = Transformer(enc_vocab_size=vocab_size,dec_vocab_size=vocab_size,\n",
    "                          d_model=d_model, n_heads=n_heads, d_ffn=d_ffn,\n",
    "                          num_layers=num_layers)\n",
    "\n",
    "output = transformer(x, y)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe86c24",
   "metadata": {},
   "source": [
    "- [Ref1](https://github.com/hyunwoongko/transformer/tree/master)\n",
    "- [Ref2](https://goyalpramod.github.io/blogs/Transformers_laid_out/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fee285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1\n",
    "EOS_ID = 2\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86d471d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a lower triangular matrix for causal masking.\"\"\"\n",
    "    return torch.tril(torch.ones((sz, sz), dtype=torch.bool))\n",
    "\n",
    "\n",
    "def create_masks(src, tgt_input, pad_id=PAD_ID):\n",
    "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)  # [B, 1, 1, S]\n",
    "    tgt_mask = (tgt_input != pad_id).unsqueeze(1).unsqueeze(2)  # [B, 1, 1, T]\n",
    "    size = tgt_input.size(1)\n",
    "    causal_mask = generate_square_subsequent_mask(size).to(tgt_input.device)  # [T, T]\n",
    "    combined_mask = tgt_mask & causal_mask.unsqueeze(0).unsqueeze(0)  # [B, 1, T, T]\n",
    "    return src_mask, combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e51ecd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_len=50):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.dataset[idx][\"en\"]\n",
    "        tgt_text = self.dataset[idx][\"de\"]\n",
    "        src_tokens = self.tokenizer.encode(src_text)[:self.max_len]\n",
    "        tgt_tokens = self.tokenizer.encode(tgt_text)[:self.max_len]\n",
    "        return {\n",
    "            \"src\": torch.tensor(src_tokens, dtype=torch.long),\n",
    "            \"tgt\": torch.tensor(tgt_tokens, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    def pad_sequence(seqs, pad_id=PAD_ID):\n",
    "        max_len = max(seq.size(0) for seq in seqs)\n",
    "        return torch.stack([\n",
    "            torch.cat([seq, torch.full((max_len - len(seq),), pad_id, dtype=torch.long)])\n",
    "            for seq in seqs\n",
    "        ])\n",
    "\n",
    "    src_seqs = [item[\"src\"] for item in batch]\n",
    "    tgt_seqs = [item[\"tgt\"] for item in batch]\n",
    "\n",
    "    return pad_sequence(src_seqs), pad_sequence(tgt_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9687fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "def averager(beta: float = 1):\n",
    "    \"\"\"\n",
    "    Returns a single function that can be called to repeatidly obtain\n",
    "    a running average from a dictionary of metrics.\n",
    "    The callback will return the new averaged dict of metrics.\n",
    "\n",
    "    `beta` is the decay parameter. If `beta == 1`, a regular running\n",
    "    average is performed. If `beta < 1`, an exponential moving average\n",
    "    is performed instead.\n",
    "    \"\"\"\n",
    "    count = defaultdict(float)\n",
    "    total = defaultdict(float)\n",
    "\n",
    "    def _update(metrics: dict, weight: float = 1) -> dict:\n",
    "        nonlocal total, count\n",
    "        for key, value in metrics.items():\n",
    "            total[key] = total[key] * beta + weight * float(value)\n",
    "            count[key] = count[key] * beta + weight\n",
    "        return {key: tot / count[key] for key, tot in total.items()}\n",
    "    return _update\n",
    "\n",
    "\n",
    "def do_epoch(epoch: int, model: nn.Module, loader: DataLoader, criterion: nn.Module, tokenizer, optimizer=None):\n",
    "    \"\"\"Run a single epoch, either in training or evaluation mode, if `optimizer` is None.\"\"\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    model.train() if optimizer is not None else model.eval()\n",
    "    average = averager()\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF()\n",
    "\n",
    "    for src, tgt in loader:\n",
    "        print(\"src\", src.shape, \"tgt\", tgt.shape)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        src_mask, tgt_mask = create_masks(src, tgt)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        with torch.set_grad_enabled(optimizer is not None):\n",
    "            prediction = model(src, tgt_input,\n",
    "                            #    src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                               )\n",
    "            print(\"prediction\", prediction.shape)\n",
    "            print(\"tgt_output\", tgt_output.shape)\n",
    "            loss = criterion(prediction.reshape(-1, prediction.size(-1)), tgt_output.reshape(-1))\n",
    "            pred_tokens = prediction.argmax(dim=-1)\n",
    "            references = [[tokenizer.decode(ref.tolist(), skip_special_tokens=True).split()] for ref in tgt_output]\n",
    "            hypotheses = [tokenizer.decode(hyp.tolist(), skip_special_tokens=True).split() for hyp in pred_tokens]\n",
    "            score = bleu.corpus_score(hypotheses=hypotheses, references=references)\n",
    "            chrf_score = chrf.corpus_score(hypotheses=hypotheses, references=references)\n",
    "\n",
    "        metrics = {\n",
    "            'loss': loss,\n",
    "            'bleu': score.score,\n",
    "            'chrf': chrf_score.score\n",
    "\n",
    "        }\n",
    "        metrics = average(metrics)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    label = 'test' if optimizer is None else 'train'\n",
    "    logger.info(f'Epoch {epoch:03d} {label: <5} summary '\n",
    "                f'loss: {metrics[\"loss\"]:.3f}, '\n",
    "                f'acc.: {metrics[\"accuracy\"]:6.2%}')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8aeac118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_ds = load_dataset(\"bentrevett/multi30k\", split=\"train\").take(10)\n",
    "    val_ds = load_dataset(\"bentrevett/multi30k\", split=\"validation\").take(5)\n",
    "\n",
    "    # Build datasets + loaders\n",
    "    train_dataset = Multi30kDataset(train_ds, tokenizer, max_len=50)\n",
    "    \n",
    "    val_dataset = Multi30kDataset(val_ds, tokenizer, max_len=50)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model\n",
    "    vocab_size = tokenizer.n_vocab\n",
    "    model = Transformer(enc_vocab_size=vocab_size, dec_vocab_size=vocab_size,\n",
    "                        d_ffn=2048, d_model=512, n_heads=8, num_layers=6).to(device)\n",
    "\n",
    "    # Loss + optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train loop\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = do_epoch(model=model,criterion=criterion, loader=train_loader, optimizer=optimizer,tokenizer=tokenizer, epoch=epoch)\n",
    "        val_loss = do_epoch(model=model,criterion=criterion, loader=val_loader,tokenizer=tokenizer, epoch=epoch)\n",
    "        print(f\"Epoch {epoch+1} / {epochs}: Train loss {train_loss:.4f} | Val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5156bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src torch.Size([2, 15]) tgt torch.Size([2, 34])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 15, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n",
      "query shape: torch.Size([2, 33, 512]) key shape: torch.Size([2, 33, 512]) value shape: torch.Size([2, 33, 512])\n",
      "query shape: torch.Size([2, 33, 512]) key shape: torch.Size([2, 15, 512]) value shape: torch.Size([2, 15, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 33, 8, 64]' is invalid for input of size 15360",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m epochs = \u001b[32m5\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     train_loss = \u001b[43mdo_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     val_loss = do_epoch(model=model,criterion=criterion, loader=val_loader,tokenizer=tokenizer, epoch=epoch)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mdo_epoch\u001b[39m\u001b[34m(epoch, model, loader, criterion, tokenizer, optimizer)\u001b[39m\n\u001b[32m     42\u001b[39m tgt_output = tgt[:, \u001b[32m1\u001b[39m:]\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     prediction = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m#    src_mask=src_mask, tgt_mask=tgt_mask,\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m, prediction.shape)\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtgt_output\u001b[39m\u001b[33m\"\u001b[39m, tgt_output.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     encoder_output = \u001b[38;5;28mself\u001b[39m.encoder(src, src_mask)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_proj(decoder_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     14\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mDecoderLayer.forward\u001b[39m\u001b[34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x + _x)\n\u001b[32m     19\u001b[39m _x = x\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmha2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x + _x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OpenSource/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mquery shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m key shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m value shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m Q = \u001b[38;5;28mself\u001b[39m.W_q(query).view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.d_head).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [B, H, T, d_head]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m K = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md_head\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     20\u001b[39m V = \u001b[38;5;28mself\u001b[39m.W_v(value).view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.d_head).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     22\u001b[39m scores = torch.matmul(Q, K.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)) / math.sqrt(\u001b[38;5;28mself\u001b[39m.d_head)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[2, 33, 8, 64]' is invalid for input of size 15360"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"bentrevett/multi30k\", split=\"train\").take(10)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = Multi30kDataset(train_ds, tokenizer, max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf342e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([ 7571,  1862,    11,  2635, 10835,   389,  2354,  1474,   867, 37413,\n",
       "            13]),\n",
       " 'tgt': tensor([   57, 42990, 10891,   469,   356,    72, 39683,    68,   337, 11033,\n",
       "            77,  1008,   264,   521,   545,  4848,  2013,   287,  4587,   399,\n",
       "         11033,   258,   410,  8207,   263,   347,  9116, 15952,    13])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb6b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
